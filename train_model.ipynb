{"cells":[{"cell_type":"markdown","metadata":{"id":"KbzZ9xe6dWwf"},"source":["## Initial setup"]},{"cell_type":"code","execution_count":1,"metadata":{"vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["0.jpg\t 167.jpg  21.jpg   310.jpg  48.jpg   530.jpg  662.jpg  746.jpg\t79.jpg\n","1.jpg\t 169.jpg  235.jpg  346.jpg  49.jpg   553.jpg  663.jpg  747.jpg\t8.jpg\n","10.jpg\t 170.jpg  236.jpg  351.jpg  497.jpg  557.jpg  664.jpg  748.jpg\t809.jpg\n","100.jpg  171.jpg  24.jpg   352.jpg  498.jpg  560.jpg  665.jpg  75.jpg\t812.jpg\n","12.jpg\t 172.jpg  240.jpg  383.jpg  499.jpg  589.jpg  667.jpg  76.jpg\t814.jpg\n","13.jpg\t 18.jpg   242.jpg  385.jpg  50.jpg   612.jpg  688.jpg  77.jpg\t816.jpg\n","136.jpg  19.jpg   25.jpg   386.jpg  501.jpg  615.jpg  690.jpg  775.jpg\t9.jpg\n","138.jpg  2.jpg\t  26.jpg   417.jpg  51.jpg   616.jpg  691.jpg  776.jpg\n","139.jpg  20.jpg   27.jpg   419.jpg  52.jpg   634.jpg  693.jpg  777.jpg\n","14.jpg\t 201.jpg  275.jpg  420.jpg  527.jpg  636.jpg  694.jpg  779.jpg\n","15.jpg\t 202.jpg  276.jpg  421.jpg  528.jpg  637.jpg  717.jpg  78.jpg\n","16.jpg\t 203.jpg  277.jpg  449.jpg  529.jpg  639.jpg  719.jpg  781.jpg\n","165.jpg  208.jpg  278.jpg  475.jpg  53.jpg   640.jpg  74.jpg   782.jpg\n"]}],"source":["#List all files in my_concept directory\n","!ls my_concept\n","\n","#Remove .ipynb_checkpoints from the my_concept directory\n","!rm -rf my_concept/.ipynb_checkpoints"]},{"cell_type":"code","execution_count":2,"metadata":{"vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-12-08 21:06:40--  https://raw.githubusercontent.com/gradient-ai/stable-diffusion/main/login.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 429 [text/plain]\n","Saving to: ‘login.py.23’\n","\n","login.py.23         100%[===================>]     429  --.-KB/s    in 0s      \n","\n","2022-12-08 21:06:45 (10.5 MB/s) - ‘login.py.23’ saved [429/429]\n","\n","Traceback (most recent call last):\n","  File \"/notebooks/login.py\", line 17, in <module>\n","    main()\n","  File \"/notebooks/login.py\", line 10, in main\n","    os.mkdir('/root/.huggingface')\n","FileExistsError: [Errno 17] File exists: '/root/.huggingface'\n"]}],"source":["#Your need a Hugging Face account and token for this to work. Info on on how to get your token here: https://huggingface.co/docs/hub/security-tokens\n","#Once you have your token, replace 'YOUR_HF_TOKEN_GOES_HERE' below with your token\n","\n","!wget https://raw.githubusercontent.com/gradient-ai/stable-diffusion/main/login.py\n","!python login.py --token YOUR_HF_TOKEN"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"30lu8LWXmg5j","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.13.0+cu116)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch) (11.10.3.66)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch) (11.7.99)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.3.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.35.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (63.1.0)\n","Collecting torch\n","  Using cached torch-1.12.0-cp39-cp39-manylinux1_x86_64.whl (776.3 MB)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.28.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (2.8)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2019.11.28)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.10)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.0\n","    Uninstalling torch-1.13.0:\n","      Successfully uninstalled torch-1.13.0\n","Successfully installed torch-1.12.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: diffusers in /usr/local/lib/python3.9/dist-packages (0.10.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.25.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (1.9.3)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.9/dist-packages (6.1.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.0)\n","Collecting torch\n","  Using cached torch-1.13.0-cp39-cp39-manylinux1_x86_64.whl (890.2 MB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.9/dist-packages (from diffusers) (4.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from diffusers) (3.7.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from diffusers) (2.28.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from diffusers) (1.23.1)\n","Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from diffusers) (0.11.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from diffusers) (9.2.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from diffusers) (2022.7.9)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy) (0.2.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch) (8.5.0.96)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch) (11.7.99)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch) (11.10.3.66)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.35.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (63.1.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata->diffusers) (3.8.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers) (2.8)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers) (1.26.10)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->diffusers) (2019.11.28)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.0\n","    Uninstalling torch-1.12.0:\n","      Successfully uninstalled torch-1.12.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.0+cu116 requires torch==1.12.0, but you have torch 1.13.0 which is incompatible.\n","torchaudio 0.12.0+cu116 requires torch==1.12.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.13.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting git+https://github.com/huggingface/diffusers.git\n","  Cloning https://github.com/huggingface/diffusers.git to /tmp/pip-req-build-asqj2hrf\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers.git /tmp/pip-req-build-asqj2hrf\n","  Resolved https://github.com/huggingface/diffusers.git to commit 3faf204c496ffed9ad48862874db7031bbe2674e\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.25.1)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.9/dist-packages (0.15.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (1.9.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from diffusers==0.10.0) (1.23.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from diffusers==0.10.0) (2.28.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from diffusers==0.10.0) (2022.7.9)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from diffusers==0.10.0) (9.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from diffusers==0.10.0) (3.7.1)\n","Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from diffusers==0.10.0) (0.11.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.9/dist-packages (from diffusers==0.10.0) (4.12.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.13.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.10.0) (4.3.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (11.7.99)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (11.10.3.66)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (8.5.0.96)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (63.1.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (0.35.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata->diffusers==0.10.0) (3.8.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers==0.10.0) (2.8)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers==0.10.0) (1.26.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->diffusers==0.10.0) (2019.11.28)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers==0.10.0) (2.1.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["#@title Install the required libs\n","!pip install -qq diffusers\n","!pip install -qq \"ipywidgets>=7,<8\"\n","!pip install -qq bitsandbytes\n","!pip install -qq accelerate\n","\n","#Install at least transformers 4.22.1\n","!pip install -qq transformers\n","!pip install -qq huggingface_hub\n","\n","#upgrade all above\n","!pip install -qq --upgrade transformers\n","!pip install -qq --upgrade diffusers\n","!pip install -qq --upgrade accelerate\n","!pip install -qq --upgrade huggingface_hub\n","\n","!pip3 install --pre torch torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n","\n","!pip install --upgrade diffusers transformers scipy ftfy torch\n","\n","#!pip install --upgrade git+https://github.com/huggingface/diffusers.git transformers accelerate scipy\n","!pip install --upgrade git+https://github.com/huggingface/diffusers.git transformers accelerate scipy\n"]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"1_h0kO-VnQog","outputId":"96e5e3cf-0da9-4a0c-b506-35de5e724d8d","vscode":{"languageId":"python"}},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /usr/local/lib/python3.9/dist-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n","  warn(f\"Failed to load image Python extension: {e}\")\n"]}],"source":["#@title Import required libraries\n","import argparse\n","import itertools\n","import math\n","import os\n","from contextlib import nullcontext\n","import random\n","\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.utils.checkpoint\n","from torch.utils.data import Dataset\n","\n","import PIL\n","from accelerate import Accelerator\n","from accelerate.logging import get_logger\n","from accelerate.utils import set_seed\n","from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel, StableDiffusionPipeline, EulerDiscreteScheduler, DPMSolverMultistepScheduler\n","#from diffusers.hub_utils import init_git_repo, push_to_hub\n","from diffusers.optimization import get_scheduler\n","from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n","from PIL import Image\n","from torchvision import transforms\n","from tqdm.auto import tqdm\n","from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n","\n","import bitsandbytes as bnb\n","\n","def image_grid(imgs, rows, cols):\n","    assert len(imgs) == rows*cols\n","\n","    w, h = imgs[0].size\n","    grid = Image.new('RGB', size=(cols*w, rows*h))\n","    grid_w, grid_h = grid.size\n","    \n","    for i, img in enumerate(imgs):\n","        grid.paste(img, box=(i%cols*w, i//cols*h))\n","    return grid"]},{"cell_type":"markdown","metadata":{"id":"Yl3r7A_3ASxm"},"source":["## Settings for teaching your new concept"]},{"cell_type":"code","execution_count":5,"metadata":{"cellView":"form","id":"If5Jswe526QP","vscode":{"languageId":"python"}},"outputs":[],"source":["#@markdown `pretrained_model_name_or_path` which Stable Diffusion checkpoint you want to use\n","pretrained_model_name_or_path = \"stabilityai/stable-diffusion-2-1-base\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":6,"metadata":{"cellView":"form","id":"8i_vLTBxAXpE","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Settings for your newly created concept\n","#@markdown `instance_prompt` is a prompt that should contain a good description of what your object or style is, together with the initializer word `sks`  \n","instance_prompt = \"a photo of starbucks coffee\" #@param {type:\"string\"}\n","#@markdown Check the `prior_preservation` option if you would like class of the concept (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time\n","prior_preservation = False #@param {type:\"boolean\"}\n","prior_preservation_class_prompt = \"a photo of a human\" #@param {type:\"string\"}\n","\n","num_class_images = 12 \n","sample_batch_size = 2\n","prior_loss_weight = 0.5\n","prior_preservation_class_folder = \"./class_images\"\n","class_data_root=prior_preservation_class_folder\n","class_prompt=prior_preservation_class_prompt"]},{"cell_type":"markdown","metadata":{"id":"NCuPKFeMLe9S"},"source":["#### Advanced settings for prior preservation (optional)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"y8SQ259CK3Cd","vscode":{"languageId":"python"}},"outputs":[],"source":["num_class_images = 12 #@param {type: \"number\"}\n","sample_batch_size = 2\n","#@markdown `prior_preservation_weight` determins how strong the class for prior preservation should be \n","prior_loss_weight = 1 #@param {type: \"number\"}\n","\n","\n","#@markdown If the `prior_preservation_class_folder` is empty, images for the class will be generated with the class prompt. Otherwise, fill this folder with images of items on the same class as your concept (but not images of the concept itself)\n","prior_preservation_class_folder = \"./class_images\" #@param {type:\"string\"}\n","class_data_root=prior_preservation_class_folder\n","\n","save_path = \"./my_concept\""]},{"cell_type":"markdown","metadata":{"id":"D633UIuGgs6M"},"source":["## Teach the model the new concept (fine-tuning with Dreambooth)\n","Execute this this sequence of cells to run the training process. The whole process may take from 15 min to 2 hours. (Open this block if you are interested in how this process works under the hood or if you want to change advanced training settings or hyperparameters)"]},{"cell_type":"code","execution_count":8,"metadata":{"cellView":"form","id":"lzxceJuASiev","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Setup the Classes\n","from pathlib import Path\n","from torchvision import transforms\n","\n","class DreamBoothDataset(Dataset):\n","    def __init__(\n","        self,\n","        instance_data_root,\n","        instance_prompt,\n","        tokenizer,\n","        class_data_root=None,\n","        class_prompt=None,\n","        size=512,\n","        center_crop=False,\n","    ):\n","        self.size = size\n","        self.center_crop = center_crop\n","        self.tokenizer = tokenizer\n","\n","        self.instance_data_root = Path(instance_data_root)\n","        if not self.instance_data_root.exists():\n","            raise ValueError(\"Instance images root doesn't exists.\")\n","\n","        self.instance_images_path = list(Path(instance_data_root).iterdir())\n","        self.num_instance_images = len(self.instance_images_path)\n","        self.instance_prompt = instance_prompt\n","        self._length = self.num_instance_images\n","\n","        if class_data_root is not None:\n","            self.class_data_root = Path(class_data_root)\n","            self.class_data_root.mkdir(parents=True, exist_ok=True)\n","            self.class_images_path = list(Path(class_data_root).iterdir())\n","            self.num_class_images = len(self.class_images_path)\n","            self._length = max(self.num_class_images, self.num_instance_images)\n","            self.class_prompt = class_prompt\n","        else:\n","            self.class_data_root = None\n","\n","        self.image_transforms = transforms.Compose(\n","            [\n","                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n","                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.5], [0.5]),\n","            ]\n","        )\n","\n","    def __len__(self):\n","        return self._length\n","\n","    def __getitem__(self, index):\n","        example = {}\n","        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n","        if not instance_image.mode == \"RGB\":\n","            instance_image = instance_image.convert(\"RGB\")\n","        example[\"instance_images\"] = self.image_transforms(instance_image)\n","        example[\"instance_prompt_ids\"] = self.tokenizer(\n","            self.instance_prompt,\n","            padding=\"do_not_pad\",\n","            truncation=True,\n","            max_length=self.tokenizer.model_max_length,\n","        ).input_ids\n","\n","        if self.class_data_root:\n","            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n","            if not class_image.mode == \"RGB\":\n","                class_image = class_image.convert(\"RGB\")\n","            example[\"class_images\"] = self.image_transforms(class_image)\n","            example[\"class_prompt_ids\"] = self.tokenizer(\n","                self.class_prompt,\n","                padding=\"do_not_pad\",\n","                truncation=True,\n","                max_length=self.tokenizer.model_max_length,\n","            ).input_ids\n","        \n","        return example\n","\n","\n","class PromptDataset(Dataset):\n","    def __init__(self, prompt, num_samples):\n","        self.prompt = prompt\n","        self.num_samples = num_samples\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","    def __getitem__(self, index):\n","        example = {}\n","        example[\"prompt\"] = self.prompt\n","        example[\"index\"] = index\n","        return example"]},{"cell_type":"code","execution_count":9,"metadata":{"cellView":"form","id":"mneZ4Ct2BenE","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Generate Class Images\n","import gc\n","if(prior_preservation):\n","    class_images_dir = Path(class_data_root)\n","    if not class_images_dir.exists():\n","        class_images_dir.mkdir(parents=True)\n","    cur_class_images = len(list(class_images_dir.iterdir()))\n","\n","    if cur_class_images < num_class_images:\n","        pipeline = StableDiffusionPipeline.from_pretrained(\n","            pretrained_model_name_or_path, revision=\"fp16\", torch_dtype=torch.float16\n","        ).to(\"cuda\")\n","        pipeline.enable_attention_slicing()\n","        pipeline.set_progress_bar_config(disable=True)\n","\n","        num_new_images = num_class_images - cur_class_images\n","        print(f\"Number of class images to sample: {num_new_images}.\")\n","\n","        sample_dataset = PromptDataset(class_prompt, num_new_images)\n","        sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=sample_batch_size)\n","\n","        for example in tqdm(sample_dataloader, desc=\"Generating class images\"):\n","            images = pipeline(example[\"prompt\"]).images\n","\n","            for i, image in enumerate(images):\n","                image.save(class_images_dir / f\"{example['index'][i] + cur_class_images}.jpg\")\n","        pipeline = None\n","        gc.collect()\n","        del pipeline\n","        with torch.no_grad():\n","          torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":10,"metadata":{"cellView":"form","id":"gIFaJum5nqeo","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Load the Stable Diffusion model\n","#@markdown Please read and if you agree accept the LICENSE [here](https://huggingface.co/CompVis/stable-diffusion-v1-4) if you see an error\n","# Load models and create wrapper for stable diffusion\n","text_encoder = CLIPTextModel.from_pretrained(\n","    pretrained_model_name_or_path, \n","    subfolder=\"text_encoder\", \n","    use_auth_token=True\n",")\n","vae = AutoencoderKL.from_pretrained(\n","        pretrained_model_name_or_path, \n","        subfolder=\"vae\", \n","        use_auth_token=True\n",")\n","unet = UNet2DConditionModel.from_pretrained(\n","        pretrained_model_name_or_path, \n","        subfolder=\"unet\", \n","        use_auth_token=True\n",")\n","tokenizer = CLIPTokenizer.from_pretrained(\n","    pretrained_model_name_or_path,\n","    subfolder=\"tokenizer\",\n","    use_auth_token=True\n",")\n","\n","scheduler = DPMSolverMultistepScheduler.from_pretrained(\n","                pretrained_model_name_or_path, \n","                subfolder=\"scheduler\"\n","                )\n","\n","                \n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"r4ayDcwEHDa4","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Setting up all training args\n","from argparse import Namespace\n","args = Namespace(\n","    pretrained_model_name_or_path=pretrained_model_name_or_path,\n","    resolution=512,\n","    center_crop=True,\n","    instance_data_dir=save_path,\n","    instance_prompt=instance_prompt,\n","    learning_rate=5e-06,\n","    max_train_steps=450,\n","    train_batch_size=1,\n","    gradient_accumulation_steps=2,\n","    max_grad_norm=1.0,\n","    mixed_precision=\"no\", # set to \"fp16\" for mixed-precision training.\n","    gradient_checkpointing=True, # set this to True to lower the memory usage.\n","    use_8bit_adam=True, # use 8bit optimizer from bitsandbytes\n","    seed=3434554,\n","    with_prior_preservation=prior_preservation, \n","    prior_loss_weight=prior_loss_weight,\n","    sample_batch_size=2,\n","    class_data_dir=prior_preservation_class_folder, \n","    class_prompt=prior_preservation_class_prompt, \n","    num_class_images=num_class_images, \n","    output_dir=\"dreambooth-concept\",\n","    num_machines=1,\n","    num_processes=2,\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"1lKGmcIyJbCu","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Training function\n","from accelerate.utils import set_seed\n","def training_function(text_encoder, vae, unet):\n","    logger = get_logger(__name__)\n","\n","    accelerator = Accelerator(\n","        gradient_accumulation_steps=args.gradient_accumulation_steps,\n","        mixed_precision=args.mixed_precision,\n","    )\n","\n","    set_seed(args.seed)\n","\n","    if args.gradient_checkpointing:\n","        unet.enable_gradient_checkpointing()\n","\n","    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n","    if args.use_8bit_adam:\n","        optimizer_class = bnb.optim.AdamW8bit\n","    else:\n","        optimizer_class = torch.optim.AdamW\n","\n","    optimizer = optimizer_class(\n","        unet.parameters(),  # only optimize unet\n","        lr=args.learning_rate,\n","    )\n","\n","    noise_scheduler = DDPMScheduler(\n","        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000\n","    )\n","    \n","    train_dataset = DreamBoothDataset(\n","        instance_data_root=args.instance_data_dir,\n","        instance_prompt=args.instance_prompt,\n","        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n","        class_prompt=args.class_prompt,\n","        tokenizer=tokenizer,\n","        size=args.resolution,\n","        center_crop=args.center_crop,\n","    )\n","\n","    def collate_fn(examples):\n","        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n","        pixel_values = [example[\"instance_images\"] for example in examples]\n","\n","        # concat class and instance examples for prior preservation\n","        if args.with_prior_preservation:\n","            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n","            pixel_values += [example[\"class_images\"] for example in examples]\n","\n","        pixel_values = torch.stack(pixel_values)\n","        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n","\n","        input_ids = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\").input_ids\n","\n","        batch = {\n","            \"input_ids\": input_ids,\n","            \"pixel_values\": pixel_values,\n","        }\n","        return batch\n","    \n","    train_dataloader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn\n","    )\n","\n","    unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, train_dataloader)\n","\n","    # Move text_encode and vae to gpu\n","    text_encoder.to(accelerator.device)\n","    vae.to(accelerator.device)\n","\n","    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n","    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n","    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n","  \n","    # Train!\n","    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n","\n","    logger.info(\"***** Running training *****\")\n","    logger.info(f\"  Num examples = {len(train_dataset)}\")\n","    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n","    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n","    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n","    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n","    # Only show the progress bar once on each machine.\n","    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n","    progress_bar.set_description(\"Steps\")\n","    global_step = 0\n","\n","    for epoch in range(num_train_epochs):\n","        unet.train()\n","        for step, batch in enumerate(train_dataloader):\n","            with accelerator.accumulate(unet):\n","                # Convert images to latent space\n","                with torch.no_grad():\n","                    latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n","                    latents = latents * 0.18215\n","\n","                # Sample noise that we'll add to the latents\n","                noise = torch.randn(latents.shape).to(latents.device)\n","                bsz = latents.shape[0]\n","                # Sample a random timestep for each image\n","                timesteps = torch.randint(\n","                    0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device\n","                ).long()\n","\n","                # Add noise to the latents according to the noise magnitude at each timestep\n","                # (this is the forward diffusion process)\n","                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n","\n","                # Get the text embedding for conditioning\n","                with torch.no_grad():\n","                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n","\n","                # Predict the noise residual\n","                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n","\n","                if args.with_prior_preservation:\n","                    # Chunk the noise and noise_pred into two parts and compute the loss on each part separately.\n","                    noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0)\n","                    noise, noise_prior = torch.chunk(noise, 2, dim=0)\n","\n","                    # Compute instance loss\n","                    loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n","\n","                    # Compute prior loss\n","                    prior_loss = F.mse_loss(noise_pred_prior, noise_prior, reduction=\"none\").mean([1, 2, 3]).mean()\n","\n","                    # Add the prior loss to the instance loss.\n","                    loss = loss + args.prior_loss_weight * prior_loss\n","                else:\n","                    loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n","\n","                accelerator.backward(loss)\n","                if accelerator.sync_gradients:\n","                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","            # Checks if the accelerator has performed an optimization step behind the scenes\n","            if accelerator.sync_gradients:\n","                progress_bar.update(1)\n","                global_step += 1\n","\n","            logs = {\"loss\": loss.detach().item()}\n","            progress_bar.set_postfix(**logs)\n","\n","            if global_step >= args.max_train_steps:\n","                break\n","\n","        accelerator.wait_for_everyone()\n","    \n","    # Create the pipeline using using the trained modules and save it.\n","    if accelerator.is_main_process:\n","        pipeline = StableDiffusionPipeline(\n","            text_encoder=text_encoder,\n","            vae=vae,\n","            unet=accelerator.unwrap_model(unet),\n","            tokenizer=tokenizer,\n","            scheduler=scheduler,\n","            safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n","            feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n","        )\n","        pipeline.save_pretrained(args.output_dir)"]},{"cell_type":"code","execution_count":13,"metadata":{"cellView":"form","id":"mfeKJn_LJi_V","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Launching training on one GPU.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d8c10088d4d7492f892d19c8050c4c4c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/450 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#@title Run training\n","import accelerate\n","accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet), num_processes=1)\n","with torch.no_grad():\n","    torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"50JuJUM8EG1h"},"source":["## Run the code with your newly trained model\n","If you have just trained your model with the code above, use the block below to run it.\n","\n","Also explore the [DreamBooth Concepts Library](https://huggingface.co/sd-dreambooth-library) "]},{"cell_type":"code","execution_count":14,"metadata":{"cellView":"form","id":"gTlUJYB1QNSN","vscode":{"languageId":"python"}},"outputs":[{"data":{"text/markdown":["## Your concept was saved successfully. [Click here to access it](https://huggingface.co/gordondavidf/starbucks-v3)\n"]},"metadata":{},"output_type":"display_data"}],"source":["#@title Save your newly created concept? you may save it privately to your personal profile or collaborate to the [library of concepts](https://huggingface.co/sd-dreambooth-library)?\n","#@markdown If you wish your model to be avaliable for everyone, add it to the public library. If you prefer to use your model privately, add your own profile.\n","\n","save_concept = True #@param {type:\"boolean\"}\n","#@markdown Once you save it you can use your concept by loading the model on any `from_pretrained` function\n","name_of_your_concept = \"starbucks-v3\" #@param {type:\"string\"}\n","where_to_save_concept = \"privately_to_my_profile\" #@param [\"public_library\", \"privately_to_my_profile\"]\n","\n","#@markdown `hf_token_write`: leave blank if you logged in with a token with `write access` in the [Initial Setup](#scrollTo=KbzZ9xe6dWwf). If not, [go to your tokens settings and create a write access token](https://huggingface.co/settings/tokens)\n","hf_token_write = \"\" #@param {type:\"string\"}\n","if(save_concept):\n","  from slugify import slugify\n","  from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n","  from huggingface_hub import create_repo\n","  from IPython.display import display_markdown\n","  api = HfApi()\n","  your_username = api.whoami()[\"name\"]\n","  pipe = StableDiffusionPipeline.from_pretrained(\n","    args.output_dir,\n","    torch_dtype=torch.float16,\n","  ).to(\"cuda\")\n","  os.makedirs(\"fp16_model\",exist_ok=True)\n","  pipe.save_pretrained(\"fp16_model\")\n","\n","  if(where_to_save_concept == \"public_library\"):\n","    repo_id = f\"sd-dreambooth-library/{slugify(name_of_your_concept)}\"\n","    #Join the Concepts Library organization if you aren't part of it already\n","    !curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX\n","  else:\n","    repo_id = f\"{your_username}/{slugify(name_of_your_concept)}\"\n","  output_dir = args.output_dir\n","  if(not hf_token_write):\n","    with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n","  else:\n","    hf_token = hf_token_write \n","\n","  readme_text = f'''---\n","license: mit\n","---\n","### {name_of_your_concept} on Stable Diffusion via Dreambooth\n","#### model by {api.whoami()[\"name\"]}\n","This your the Stable Diffusion model fine-tuned the {name_of_your_concept} concept taught to Stable Diffusion with Dreambooth.\n","It can be used by modifying the `instance_prompt`: **{instance_prompt}**\n","\n","You can also train your own concepts and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n","And you can run your new concept via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-concepts)\n","\n","'''\n","  #Save the readme to a file\n","  readme_file = open(\"README.md\", \"w\")\n","  readme_file.write(readme_text)\n","  readme_file.close()\n","  #Save the token identifier to a file\n","  text_file = open(\"token_identifier.txt\", \"w\")\n","  text_file.write(instance_prompt)\n","  text_file.close()\n","  operations = [\n","    CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n","    CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n","  ]\n","  create_repo(repo_id,private=True, token=hf_token)\n","  \n","  api.create_commit(\n","    repo_id=repo_id,\n","    operations=operations,\n","    commit_message=f\"Upload the concept {name_of_your_concept} embeds and token\",\n","    token=hf_token\n","  )\n","  \n","  api.upload_folder(\n","    folder_path=\"fp16_model\",\n","    path_in_repo=\"\",\n","    repo_id=repo_id,\n","    token=hf_token\n","  )\n","  \n","  # api.upload_folder(\n","  #   folder_path=save_path,\n","  #   path_in_repo=\"concept_images\",\n","  #   repo_id=repo_id,\n","  #   token=hf_token\n","  # )\n","\n","display_markdown(f'''## Your concept was saved successfully. [Click here to access it](https://huggingface.co/{repo_id})\n","''', raw=True)"]},{"cell_type":"code","execution_count":15,"metadata":{"cellView":"form","id":"2CMlPbOeEC09","vscode":{"languageId":"python"}},"outputs":[],"source":["#@title Set up the pipeline \n","try:\n","    pipe\n","except NameError:\n","    pipe = StableDiffusionPipeline.from_pretrained(\n","        args.output_dir,\n","        torch_dtype=torch.float16,\n","    ).to(\"cuda\")"]},{"cell_type":"code","execution_count":16,"metadata":{"vscode":{"languageId":"python"}},"outputs":[{"data":{"text/plain":["37"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["#Clear memory\n","torch.cuda.empty_cache()\n","import gc\n","gc.collect()\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["NCuPKFeMLe9S","D633UIuGgs6M"],"machine_shape":"hm","provenance":[{"file_id":"https://github.com/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb","timestamp":1665953125463}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"vscode":{"interpreter":{"hash":"006d6a5aa44df4b7a65f96b57dcbf9090a9d32a40585edf21ad1eeeb89863a27"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"01b6cb52e86a4b69ae11c0a10d7688f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0862d6ac9a774d958b07f5b3c7d39726":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a4f5827c6694322aac53eef58361be4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"169f9d7cfb60460c91fa301dc07b88cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"PasswordModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_2f1bd58d757b4755bf6e43dcd1db7a8d","placeholder":"​","style":"IPY_MODEL_6407a100ee1a48fc946c7968160d8c11","value":""}},"16e498bcf934417fb737ff32bfd00f57":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f1bd58d757b4755bf6e43dcd1db7a8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f6dd40cc3d34b1c934b186ff1cb1bc5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"302c0ec13bc84d7c9d328e93b046b4fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_79c86255caf1493d8660ace4fa5738cb","IPY_MODEL_576424660f224261b366d713967deb94","IPY_MODEL_fb70a01286694e1fb40dfd6c940728e1"],"layout":"IPY_MODEL_826d782955cd4dc0bd56c81c5d944330"}},"318b6ba49b324a02be064792c61da1f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_329e8da97d0044aca10a821dc3154fea","placeholder":"​","style":"IPY_MODEL_7b06663e76fd4e00abcaeb1aa5e0c415","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"329e8da97d0044aca10a821dc3154fea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"333cc2b099274f25af41f54753ba9305":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3fc16e2f87f041d282b708b67a443875":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f6dd40cc3d34b1c934b186ff1cb1bc5","placeholder":"​","style":"IPY_MODEL_0862d6ac9a774d958b07f5b3c7d39726","value":"100%"}},"45f2700e51eb49a98f305db44c378cf4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_318b6ba49b324a02be064792c61da1f1","IPY_MODEL_169f9d7cfb60460c91fa301dc07b88cc","IPY_MODEL_cd65142ea5004475b34a830ccc7db6cd","IPY_MODEL_7267876e603a400da469cb87f7c446c4"],"layout":"IPY_MODEL_fea860f9ed9246189b71717b9b13be20"}},"5135e0a705fa4a93a929546feed88f0d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"531fccae2fd741dab7b02f4a1ac17f21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5135e0a705fa4a93a929546feed88f0d","placeholder":"​","style":"IPY_MODEL_01b6cb52e86a4b69ae11c0a10d7688f8","value":" 51/51 [00:14&lt;00:00,  3.44it/s]"}},"5543bfa403a14af7a572f943d0359336":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"576424660f224261b366d713967deb94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d63afe1f5ee4b6a9951058b419d8059","max":51,"min":0,"orientation":"horizontal","style":"IPY_MODEL_333cc2b099274f25af41f54753ba9305","value":51}},"5994e37481214ea7aa27415b7c4edc43":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6407a100ee1a48fc946c7968160d8c11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7267876e603a400da469cb87f7c446c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce7210f7a79d449e8709fd700dda49ce","placeholder":"​","style":"IPY_MODEL_0a4f5827c6694322aac53eef58361be4","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"79c86255caf1493d8660ace4fa5738cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5994e37481214ea7aa27415b7c4edc43","placeholder":"​","style":"IPY_MODEL_e4a4ce8dba4a40bbb246eebb3f3aaef7","value":"100%"}},"7b06663e76fd4e00abcaeb1aa5e0c415":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80879a82806b4be8a0f6a4ac111026df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_16e498bcf934417fb737ff32bfd00f57","max":51,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e8258f8326554c958923abd3323a33d6","value":51}},"826d782955cd4dc0bd56c81c5d944330":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d63afe1f5ee4b6a9951058b419d8059":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c6d5ee3e13d449296a13096ea81f7db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a497ef1b787f40ec8dc6b523a15b7ce5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"a92e5369635b45ea97bc8238adf15c7c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd65142ea5004475b34a830ccc7db6cd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_a92e5369635b45ea97bc8238adf15c7c","style":"IPY_MODEL_a497ef1b787f40ec8dc6b523a15b7ce5","tooltip":""}},"ce7210f7a79d449e8709fd700dda49ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4a4ce8dba4a40bbb246eebb3f3aaef7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e615aa96bb8d461cb6da7384d8180737":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8258f8326554c958923abd3323a33d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f54f189db8614f28a5bc262a3dda4006":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3fc16e2f87f041d282b708b67a443875","IPY_MODEL_80879a82806b4be8a0f6a4ac111026df","IPY_MODEL_531fccae2fd741dab7b02f4a1ac17f21"],"layout":"IPY_MODEL_9c6d5ee3e13d449296a13096ea81f7db"}},"fb70a01286694e1fb40dfd6c940728e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5543bfa403a14af7a572f943d0359336","placeholder":"​","style":"IPY_MODEL_e615aa96bb8d461cb6da7384d8180737","value":" 51/51 [00:14&lt;00:00,  3.39it/s]"}},"fea860f9ed9246189b71717b9b13be20":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}}}}},"nbformat":4,"nbformat_minor":0}
